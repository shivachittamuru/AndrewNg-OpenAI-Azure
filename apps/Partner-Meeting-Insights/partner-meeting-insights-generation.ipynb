{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import tiktoken\n",
    "import os, json, time\n",
    "import docx2txt\n",
    "\n",
    "# to address rate limits with Azure OpenAI API\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential # for exponential backoff\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "model = os.environ['CHAT_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_file_name = \"./data/PowerSchoolMicrosoft Architecture Design Session_2023-06-20.docx\" \n",
    "txt_file_name = './data/final-transcript.txt'\n",
    "\n",
    "with open(doc_file_name, 'rb') as infile:\n",
    "    outfile = open(txt_file_name, 'w', encoding='utf-8')\n",
    "    doc = docx2txt.process(infile)\n",
    "    lines = doc.splitlines()\n",
    "    transcript = \"\"\n",
    "    for line in lines:\n",
    "        if '-->' in line:\n",
    "            continue\n",
    "        elif line.strip().isdigit():\n",
    "            transcript += '\\n\\n'\n",
    "        else:\n",
    "            transcript += line.strip() + '\\n'  \n",
    "    \n",
    "    \n",
    "    outfile.write(transcript)\n",
    "\n",
    "    outfile.close()\n",
    "    infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks: 12 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "llm = AzureChatOpenAI(temperature=0.2,\n",
    "    openai_api_base=openai.api_base,\n",
    "    openai_api_version=openai.api_version,\n",
    "    deployment_name=model,\n",
    "    openai_api_key=openai.api_key,\n",
    "    openai_api_type = openai.api_type,\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 5000, chunk_overlap  = 200, length_function = len)\n",
    "texts = text_splitter.split_text(transcript)\n",
    "print(f\"Number of Chunks: {len(texts)} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langchain Version for generating a summary of a long document\n",
    "\n",
    "# docs = [Document(page_content=t) for t in texts]\n",
    "# chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "# chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=30), stop=stop_after_attempt(10))\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\", temperature=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "        max_tokens=150,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for text in texts:\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following conversation, delimited by triple backticks, without missing out on any important details. Include people's names as needed based on the context. \n",
    "    Conversation: ```{text}```\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt)\n",
    "    summaries.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hans-Peter Acker asks if everyone is aware of the meeting's purpose and suggests a quick introduction. Arka Chakraborty introduces his team from Power School, including Harshit, Gayatri, Gaurav, and Vishnu. Hans-Peter Acker introduces Catherine from Microsoft and Ellie and Shiva from GPS US. Ellie explains that the meeting is for an architecture design session and asks Power School to share their work. Arka explains that they work on proof of concepts and collaborate with the product team. He shares two examples of their work: question generation and a charting app. They are currently working on integrating these features with Power School's established products.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/long_summary.txt', 'w') as file:\n",
    "    for summary in summaries:\n",
    "        file.write(summary + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The meeting is for an architecture design session\n",
      "- Arka Chakraborty introduces his team from Power School\n",
      "- Power School works on proof of concepts and collaborates with the product team\n",
      "- Power School shares two examples of their work: question generation and a charting app\n",
      "- Power School is currently working on integrating these features with their established products\n",
      "- Power School discusses their analytics products, including Unified Insights and Connected Intelligence\n",
      "- Power School is exploring the use of generative AI to create charts through a UI interface\n",
      "- Power School is working on a Power Tutor app that teaches students and provides evaluation feedback\n",
      "- The architecture for the app is discussed, with technical difficulties experienced during screen sharing\n",
      "- The group discusses various metrics and categories for quality checks\n",
      "- Power School plans to replace GPT 4 with other looms and their AWS bedrock or even open source LMS\n",
      "- The group discusses security and guardrails around the API call\n",
      "- The group discusses prompt engineering and the use of prompts generated by Catherine\n",
      "- The group discusses the API endpoint and token generation system for security\n",
      "- The group discusses the need to prevent irrelevant questions and suggests grounding the model in databases and data sources\n",
      "- The group discusses the need to reference benchmark material for accuracy in their work\n",
      "- The group discusses investigating different rag options and techniques for bias and sensitivity, content filtering, and abuse monitoring\n",
      "- The group discusses performance testing and evaluation, question generation, and ways to monitor and measure quality\n",
      "- The team agrees to have regular calls, potentially weekly, to discuss progress\n",
      "- The team discusses how they will advise on the areas they identified and communicate via Teams\n",
      "- The team discusses the possibility of doing hackathons and sharing code snippets\n",
      "- The team discusses the development of a Power Tutor app using generative AI and Open AI calls\n",
      "- The team discusses the plan to develop an M Fe micro front end with the back end so that any of the product teams can embed the tutor within the power school applications\n",
      "- The team plans to deploy it in a serverless manner\n",
      "- The team plans to share their work and schedule a deep dive session on one of the use cases\n",
      "- The team discusses technical and legal aspects of the project\n",
      "- The team plans to identify specific tasks where Microsoft can help\n"
     ]
    }
   ],
   "source": [
    "final_prompt = f\"\"\"\n",
    "Extract and list the key discussion items from the following meeting notes, delimited by triple backticks, without missing out any important details. \n",
    "Meeting Notes: ```{\" \".join(summaries)}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(final_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Discussion Items:\n",
      "- Introduction of attendees\n",
      "- Power School's proof of concept work on question generation and charting app integration\n",
      "- Power School's analytics products, including Unified Insights and Connected Intelligence\n",
      "- Architecture for Power Tutor app integration with established products\n",
      "- Technical difficulties with screen sharing\n",
      "- Quality checks and prompt engineering\n",
      "- Engagement with each product and deep dive sessions\n",
      "- Fact-checking and benchmark material\n",
      "- Design and security measures\n",
      "- Advising on identified areas and regular meetings\n",
      "- Development of Power Tutor app using generative AI and Open AI calls\n",
      "- M Fe micro front end development for embedding tutor within Power School applications\n",
      "- Legalities and logistics of sharing work\n",
      "\n",
      "Key Takeaways:\n",
      "- The meeting was for an architecture design session\n",
      "- Power School is working on integrating their proof of concept work with established products\n",
      "- Power School has analytics products that use student and school data to create charts and graphs\n",
      "- The architecture for the Power Tutor app will depend on which established product it integrates with\n",
      "- The group discussed quality checks, prompt engineering, fact-checking, and benchmark material\n",
      "- They plan to have regular meetings to discuss progress and do deep dive sessions on use cases\n",
      "- They are developing a Power Tutor app using generative AI and Open AI calls, and plan to deploy it in a serverless manner\n",
      "- They will need to address legalities and logistics when sharing work\n",
      "- GitHub or SharePoint may be used to share code\n"
     ]
    }
   ],
   "source": [
    "final_prompt = f\"\"\"\n",
    "List out the key discussion items and key takeaways in a structured format from the following meeting notes, delimited by triple backticks. \n",
    "Meeting Notes: ```{\" \".join(summaries)}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(final_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meeting notes describe a conversation between various teams working on a project related to generative AI and question generation. Arka Chakraborty introduces his team from Power School, including lead engineers and data scientists, and discusses their approach to proof of concepts and gives examples of their work, including question generation and a charting app. They plan to collaborate with the Performance Matters Engineering team to integrate their features. \n",
      "\n",
      "The team discusses the architecture for the question generation app, which will also be integrated with an established platform. They are not too concerned about the architecture and are focused on generative AI. There are some technical difficulties with screen sharing and inviting team members to the call. The conversation involves a group discussing a pipeline for generating questions using GPT 4. They are having trouble sharing diagrams and screens, but eventually, Gayathri is able to share a diagram of the pipeline. \n",
      "\n",
      "Harshit Nyati explains that the pipeline uses two endpoints to get responses from Azure API, and the generated questions are saved into a database. There is also an asynchronous job that performs quality checks on the questions using metrics such as depth of knowledge and blooms taxonomy. The group is still researching and developing these metrics. \n",
      "\n",
      "The team discusses the topics covered, the plan to replace GPT 4 with other tools, and the process for getting feedback from users. They also discuss the security measures in place and the grounding of the questions in data sources. The team mentions using Lang Chain and eval chain, and the conversation ends with a discussion on security measures and the API endpoint. \n",
      "\n",
      "The team discusses the need for guardrails around the content generated by the generative AI. The team talks about the importance of grounding the model in databases and data sources to ensure that it only uses sources of truth that are relevant to the application. They also discuss the use of a preliminary classifier to validate that the prompt is for the correct use case. \n",
      "\n",
      "The team discusses techniques for content filtering, abuse monitoring, bias and sensitivity, and performance testing and evaluation. They are also looking for ways to monitor and measure the quality of the questions and content they are generating. The team is open to recommendations and techniques from others to improve their project. \n",
      "\n",
      "The team discusses communication methods, including using Teams and email, and how they will provide support through code snippets and hackathons. They also talk about the specific use cases they will focus on and the areas where they need the most help. \n",
      "\n",
      "The team discusses using established architectural practices and NFRs, and the need for help with generative AI and Open AI calls. The team also discusses the difference between the Build With and Code With teams, with Build With handling prototyping and architecture design, and Code With handling production and shipping of solutions. They briefly touch on the Power Tutor app and its architecture. \n",
      "\n",
      "The team is planning to do mini evaluations or feedbacks within the tutor and is looking for help with content filtering and quality of teaching. The response time is short, so a serverless architecture is being considered. The team will share their experiments before the other team provides guidance. The two teams will schedule a deep dive session on one of the use cases and share the work they have done in those areas. \n",
      "\n",
      "The conversation ends with everyone saying goodbye and thanking each other.\n"
     ]
    }
   ],
   "source": [
    "final_prompt = f\"\"\"\n",
    "Provide a comprehensive explanation of the demo made in the following meeting notes, delimited by triple backticks, without missing out on any important details. \n",
    "Meeting Notes: ```{\" \".join(summaries)}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(final_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openaiworkshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
